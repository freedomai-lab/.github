
<!--
![opengvlab stars](https://img.shields.io/github/stars/opengvlab?style=social)(https://github.com/opengvlab) + [![Alpha-VLLM stars](https://img.shields.io/github/stars/Alpha-VLLM?style=social)](https://github.com/Alpha-VLLM) + [![uni-medical stars](https://img.shields.io/github/stars/uni-medical?style=social)](https://github.com/uni-medical) 
[![Twitter](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Fopengvlab)](https://twitter.com/opengvlab)
-->


![Static Badge](https://img.shields.io/badge/Stars-50k-blue?style=social&logo=github)
[![Twitter](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Fopengvlab)](https://twitter.com/opengvlab)

## Welcome to OpenGVLab! ğŸ‘‹

We are a research group from Shanghai AI Lab focused on Vision-Centric AI research. The GV in our name, OpenGVLab, means general vision, a general understanding of vision, so little effort is needed to adapt to new vision-based tasks.

We develop model architecture and release pre-trained foundation models to the community to motivate further research in this area. We have made promising progress in general vision AI, with ***109 SOTA***ğŸš€. In 2022, our open-sourced foundation model 65.5 mAP on the COCO object detection benchmark, 91.1% Top1 accuracy in Kinetics 400, achieved landmarks for AI visionğŸ‘€ tasks for imageğŸ–¼ï¸ and videoğŸ“¹ understanding. In 2023, we created [VideoChat](https://github.com/OpenGVLab/Ask-Anything)ğŸ¦œ,[llama-adapter](https://github.com/OpenGVLab/llama-adapter)ğŸ¦™, 3D foundation model [Ponder V2](https://github.com/OpenGVLab/PonderV2)ğŸ§Š and many more wonderful works! In **CVPR 2023**, our vision foundation model [InternImage](https://github.com/OpenGVLab/internimage) was listed as one of the most influential papers, and by benefiting from our partner [OpenDriveLab](https://github.com/opendrivelab), we won the [Best paper](https://github.com/opendrivelab/UniAD) togetherğŸ‰ . 

In **2024**, we released the **best open-source VLM** [InternVL](https://github.com/OpenGVLab/internvl) , video understanding foundation model [InternVideo2](https://github.com/OpenGVLab/internvideo), which won 7 Champions on [EgoVis challenges](https://github.com/OpenGVLab/EgoVideo) ğŸ¥‡. Up to now, our brilliant team have open-sourced more than 70 works, please [find them here](https://github.com/orgs/OpenGVLab/repositories)ğŸ˜ƒ

Based on solid vision foundations, we have expanded to Multi-Modality models and. We aim to empower individuals and businesses by offering a higher starting point for developing vision-based AI products and lessening the burden of building an AI model from scratch.

Branchesï¼š [Alpha](https://github.com/Alpha-VLLM) (explore lattest advances in vision+language research), [uni-medical](https://github.com/uni-medical) (focus on medical AI), [Vchitect](https://github.com/vchitect) (Generative AIï¼‰

 Follow us: &nbsp;&nbsp;  ![Twitter X logo](./twitter-x-logo.svg) [Twitter](https://twitter.com/opengvlab) &nbsp;&nbsp;ğŸ¤—[Hugging Face](https://huggingface.co/OpenGVLab) &nbsp;&nbsp;  ![Medium logo](./medium.png) [Medium](https://medium.com/@opengvlab) &nbsp;&nbsp; ![WeChat logo](./wechat.png) [WeChat](./opengv-wechat.jpeg) &nbsp;&nbsp;  ![zhihu logo](./zhihu.png) [Zhihu](https://www.zhihu.com/org/opengvlab)
